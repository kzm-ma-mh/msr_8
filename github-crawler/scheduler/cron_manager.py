"""
Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ Ø¯ÙˆØ±Ù‡â€ŒØ§ÛŒ
"""

from __future__ import annotations

import signal
import sys
import time
from datetime import datetime

import schedule

from config.settings import CRON_INTERVAL_HOURS, PROJECTS_PER_KEYWORD, SEARCH_KEYWORDS
from core.data_extractor import DataExtractor
from core.gitea_migrator import GiteaMigrator
from core.github_crawler import GitHubCrawler
from models.repository import RepositoryDB
from utils.logger import log


class CronManager:
    """Ø§Ø¬Ø±Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒâ€ŒØ´Ø¯Ù‡ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ†"""

    def __init__(self):
        self.db = RepositoryDB()
        self.crawler = GitHubCrawler(self.db)
        self.extractor = DataExtractor(self.db, self.crawler.rate_limiter)
        self.migrator = GiteaMigrator(self.db)
        self._running = True

        signal.signal(signal.SIGINT, self._shutdown)
        signal.signal(signal.SIGTERM, self._shutdown)

    def _shutdown(self, signum, frame):
        log.info("\nğŸ›‘ Ø®Ø§Ù…ÙˆØ´ÛŒ Ø§ÛŒÙ…Ù†...")
        self._running = False
        sys.exit(0)

    def run_full_pipeline(self) -> None:
        """Ú©Ø±ÙˆÙ„ â†’ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ â†’ Ø§Ø³ØªØ®Ø±Ø§Ø¬ â†’ Ø§Ù†ØªÙ‚Ø§Ù„"""
        start = datetime.utcnow()
        log.info("\n" + "=" * 60)
        log.info(
            f"ğŸš€ [bold magenta]Ø´Ø±ÙˆØ¹ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ†[/] â€” "
            f"{start.strftime('%Y-%m-%d %H:%M:%S')} UTC"
        )
        log.info(
            f"ğŸ“‹ Ù‡Ø¯Ù: {PROJECTS_PER_KEYWORD} Ù¾Ø±ÙˆÚ˜Ù‡ Ã— "
            f"{len(SEARCH_KEYWORDS)} Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ = "
            f"{PROJECTS_PER_KEYWORD * len(SEARCH_KEYWORDS)} Ù¾Ø±ÙˆÚ˜Ù‡"
        )
        log.info("=" * 60)

        try:
            # â”€â”€ Rate Limit â”€â”€
            self.crawler.rate_limiter.check_rate_limit()

            # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û±: Ú©Ø±ÙˆÙ„ + Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ â”€â”€
            log.info("\nğŸ“¡ [bold]Ù…Ø±Ø­Ù„Ù‡ Û±: Ú©Ø±ÙˆÙ„ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ[/]")
            valid_repos = self.crawler.search_repositories()

            # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û²: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ â”€â”€
            log.info("\nğŸ“¥ [bold]Ù…Ø±Ø­Ù„Ù‡ Û²: Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ[/]")
            for i, repo in enumerate(valid_repos, 1):
                log.info(f"\nâ”€â”€ [{i}/{len(valid_repos)}] â”€â”€")
                self.extractor.extract_all(repo)
                time.sleep(1)

            # â”€â”€ Ù…Ø±Ø­Ù„Ù‡ Û³: Ø§Ù†ØªÙ‚Ø§Ù„ â”€â”€
            log.info("\nğŸš€ [bold]Ù…Ø±Ø­Ù„Ù‡ Û³: Ø§Ù†ØªÙ‚Ø§Ù„ Ø¨Ù‡ Gitea[/]")
            migration = self.migrator.migrate_all_pending()

            # â”€â”€ Ú¯Ø²Ø§Ø±Ø´ â”€â”€
            elapsed = (datetime.utcnow() - start).total_seconds()
            stats = self.db.get_stats()

            log.info("\n" + "=" * 60)
            log.info("ğŸ“Š [bold green]Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ[/]")
            log.info(f"   â±ï¸  Ù…Ø¯Øª Ø§Ø¬Ø±Ø§: {elapsed:.0f} Ø«Ø§Ù†ÛŒÙ‡ ({elapsed/60:.1f} Ø¯Ù‚ÛŒÙ‚Ù‡)")
            log.info(f"   ğŸ” Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ø¬Ø¯ Ø´Ø±Ø§ÛŒØ·: {len(valid_repos)}")
            log.info(f"   âœ… Ø§Ù†ØªÙ‚Ø§Ù„ Ù…ÙˆÙÙ‚: {migration['success']}")
            log.info(f"   âŒ Ø§Ù†ØªÙ‚Ø§Ù„ Ù†Ø§Ù…ÙˆÙÙ‚: {migration['failed']}")
            log.info(f"   ğŸ—„ï¸  Ú©Ù„ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {stats['total_repos']}")
            log.info(f"   âœ… Ø¢Ù…Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´: {stats['training_ready']}")
            log.info(f"   â›” Ø±Ø¯ Ø´Ø¯Ù‡: {stats['rejected']}")
            log.info(f"   ğŸ“„ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡: {stats['total_extracted_records']}")
            if stats["extracted_by_type"]:
                for dtype, count in stats["extracted_by_type"].items():
                    log.info(f"      {dtype}: {count}")
            log.info("=" * 60)

        except Exception as e:
            log.exception(f"âŒ Ø®Ø·Ø§ÛŒ Ù¾Ø§ÛŒÙ¾Ù„Ø§ÛŒÙ†: {e}")

    def start_scheduler(self) -> None:
        """Ø´Ø±ÙˆØ¹ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯"""
        log.info(f"â° Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯: Ù‡Ø± {CRON_INTERVAL_HOURS} Ø³Ø§Ø¹Øª")

        self.run_full_pipeline()

        schedule.every(CRON_INTERVAL_HOURS).hours.do(self.run_full_pipeline)
        log.info(f"â° Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒ: {schedule.next_run()}")

        while self._running:
            schedule.run_pending()
            time.sleep(30)